import pandas as pd
import re
import string
import nltk


with open('output.txt') as f:
	log = f.readlines()

text_article = ''
for line in log:
	text_article += line
    

#print(text_article)


print(string.punctuation)

#pre-processing of raw data
#remove punctuation


def remove_punctuation(txt):
    txt_nopunt = "".join([c for c in txt if c not in string.punctuation])
    return txt_nopunt
 
import pandas as pd
import re
import string
import nltk


with open('output.txt') as f:
	log = f.readlines()

text_article = ''
for line in log:
	text_article += line
    
print(text_article)


stopwords = nltk.corpus.stopwords.words('english')
ps = nltk.PorterStemmer()


#print(raw_data[0:500])

#print(sent_tokenize(raw_data))

tokenised_data = word_tokenize(raw_data)
#print(tokenised_data)

#for i in word_tokenize(raw_data):
    #print (i)

parsed_data = raw_data.replace('\t', '\n').split('\n')
parsed_data[0:10]
#type(parsed_data)

#print(parsed_data)


parsed_data1 = remove_punctuation(text_article)
#print(parsed_data1)

#Text cleaning: remove stop words.
#stopwords = nltk.corpus.stopwords.words('english')
#ps = nltk.PorterStemmer()

'''
def remove_punctuation(txt):
    txt_nopunt = "".join([c for c in txt if c not in string.punctuation])
    tokens = re.split('\W+', txt)
    txt = " ".join([ps.stem(word) for word in tokens if word not in stopwords])
    return txt_nopunt
''' 

stopwords = nltk.corpus.stopwords.words('english')
ps = nltk.PorterStemmer()


from nltk.tokenize import word_tokenize
tokens = word_tokenize(parsed_data1)
print('this is tokens', tokens[:100])


#Filter out stop words

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print('this is stop_words', stop_words)

# remove remaining tokens that are not alphabetic
words = [word for word in tokens if word.isalpha()]
# filter out stop words

words = [w for w in tokens if not w in stop_words]
print('this is words', words[:100])

#https://machinelearningmastery.com/clean-text-machine-learning-python/


